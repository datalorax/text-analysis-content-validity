---
title: "Evaluating Content-Related Validity Evidence Using a Text-Based, Machine Learning Procedure"
shorttitle: "Text Modeling and Validity"
date: "`r format(Sys.time(), '%B %d, %Y')`"

author: 
  - name          : "Daniel Anderson"
    affiliation   : "1"
    corresponding : yes
    email         : "daniela@uoregon.edu"
    address       : "5262 University of Oregon"
  - name          : "Brock Rowley"
    affiliation   : "1"
    email         : "brockr@uoregon.edu"
  - name          : "P. Shawn Irvin"
    affiliation   : "1"
    email         : "pirvin@uoregon.edu"
  - name          : "Joshua M. Rosenberg"
    affiliation   : "2"
    email         : "jmrosenberg@utk.edu"
  - name          : "Sondra Stegenga"
    affiliation   : "1"
    email         : "sondras@uoregon.edu"
affiliation:
  - id            : "1"
    institution   : "University of Oregon"
  - id            : "2"
    institution   : "University of Tennessee, Knoxville"

bibliography      : refs.bib

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

abstract: |
  The alignment of test items with content standards is a critical source of
  content-related validity evidence within high-stakes testing and 
  accountability frameworks. Typically, alignment studies are conducted with
  panels of experts providing qualitative judgments on the degree of alignment
  with each item in the test and the representative content standards, from
  which various summary statistics are then calculated [e.g., categorical
  concurrence, balance of representation; @webb99]. In this paper, we propose an
  approach that builds upon traditional methods for alignment by leveraging
  text-based, machine learning procedures, specifically topic modeling, to
  identify text-based clusters, or topics, within the content standards. The
  probability that each item from a statewide assessment aligns with each
  cluster/topic can then be estimated as an additional source of content-related
  validity evidence. We discuss the utility of this approach, and show how
  visualizations can be used to evaluate the overall coverage of the content
  standards by the test items.
class             : "man, fleqn, noextraspace"
header-includes:
    - \raggedbottom
    - \setlength{\parskip}{0pt}
output            : papaja::apa6_pdf
---

```{r setup, include=FALSE}
library(tidyverse)
library(rio)
library(here)
library(janitor)
library(tidytext)
library(topicmodels)
library(ldatuning)
library(scales)
library(patchwork)
#devtools::install_github("dgrtwo/drlib")
library(drlib)
library(extrafont)

#font_import()

theme_set(theme_minimal(20)  +
  theme(strip.text.x = element_text(family = "Times New Roman"),
        plot.title = element_text(family = "Times New Roman"),
        axis.text = element_text(color = "gray20",
                                 face = "plain",
                                 family = "Times New Roman"),
        axis.title = element_text(color = "gray20",
                                  family = "Times New Roman",
                                  face = "plain"),
        legend.text = element_text(family = "Times New Roman"),
        legend.title = element_text(family = "Times New Roman")))

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      dev = "png",
                      dpi = 700)
```

```{r load_data}
standards <- import(here("data", "8gradescience.xlsx"), 
                    setclass = "tbl_df") %>% 
  clean_names()

webbwords <- import(here("data", "stopwords-webb.xlsx"),
                    setclass = "tbl_df")

items <- import(here("data", "G8_Sci_Items.xlsx"),
                setclass = "tbl_df") %>% 
  clean_names()
```

```{r doc_term_matrices}
# Create a document term matrix for standards
dtm <- standards %>% 
  select(code, ngss_standard) %>% 
  unnest_tokens(word, ngss_standard) %>% 
  anti_join(stop_words) %>% 
  anti_join(webbwords) %>% 
  filter(word != "explanation", # Filter out any additional words we don't want in there
         word != "results") %>% 
  group_by(code) %>% 
  count(word) %>% 
  ungroup() %>% 
  cast_dtm(code, word, n)

# Create a document term matrix for items 
idtm <- items %>% 
  select(item_id, standards, prompt, option_a, option_b, option_c) %>% 
  unite(text, prompt, option_a, option_b, option_c, sep = " ") %>%
  unnest_tokens(word, text) %>%
  filter(word != "a",
         word != "b",
         word != "c") %>%
  anti_join(stop_words) %>% 
  anti_join(webbwords) %>% 
  group_by(item_id) %>% 
  count(word) %>% 
  ungroup() %>% 
  cast_dtm(item_id, word, n)
```

For the past two decades, the landscape of education has evolved through the passage of legislation aimed at improving student outcomes through standards-driven accountability such as the No Child Left Behind Act [@nclb02] and its more recent renewal as the Every Student Succeeds Act [see @mcguinn16]. This has led to a substantial increase in standardized educational assessment use. As part of this movement, an intensive focus on alignment has also emerged due to the need for congruence between assessments, standards, curricula, and instruction to accurately and effectively measure, evaluate, and impact student outcomes [e.g., @porter02]. 

The goal of alignment is to "create a coherent educational system that conveys a clear and unified message about expectations and goals" [@vockley09, p.8]. Alignment studies are commonly conducted with standards-based assessments as a means of evaluating content-related validity evidence, with panels of experts judging the linkage between the content represented in the standards and the content represented in the test items [@sireci07; @webb97]. Content-related validity evidence is one of the five major sources of validity evidence outlined by the *Standards for Educational and Psychological Testing* [@standards14] and is a critical component of the "overall evaluative judgment" [@messick95, p. 741] of the validity of test scores for a given use. However, alignment studies are difficult to design and execute even under ideal conditions. The number of participants, methodology used to build consensus among professional judgments, group dynamics, analyses, and costs associated with time and travel are all elements for consideration [@anderson15]. 

According to @kane06, content validity is necessarily based on judgments, is more subject to confirmation bias, and lacks the objectivity of other sources of validity evidence (such as criterion-related evidence). Ideally, content-experts would establish consensus about the alignment and content-representativeness of the items within a test to a given set of content standards. In practice, however, group dynamics can often play a role [as mentioned by @anderson15], and the evidence acquired may depend upon the specific group of experts providing the judgment. 

This paper adds to the literature on content-related validity evidence through
an initial exploration into the use of a text-based machine learning algorithm.
Specifically, we evaluate the *textual congruence* between a set of content
standards and the text represented in the items of a high-stakes test used
within a statewide accountability system. We view the evidence gained from these analyses as a triangulating and supplemental source of content-related validity evidence that may also be useful for diagnostic purposes during test development. In the following section, we review prior research using similar methods and describe the potential of these approaches in contributing to the evaluation of content-related validity evidence.

# Background
A few recent studies have examined the use of text-based computational
approaches for gathering validity evidence for a given measure. For example,
@sherin13 used a simple Natural Language Processing technique to identify common
groups of responses to transcribed interview answers about a scientific
phenomenon. @beggrow14 employed a similar approach to examine the alignment
between groups of responses identified from a computational method and manual 
(human) codes of the same responses, finding that the computational approach
could identify students’ conceptual understanding as veridically as human-coded
responses, or responses from other data sources, such as audio data. These
studies demonstrate the potential of computational methods for contributing to
the overall construct validity evidence of measures of complex phenomena and
processes.

Topic modeling, one such computational method, is a probabilistic method for identifying latent topics in text-based data. Its history stems from qualitative content analysis and latent semantic analysis [@mohr13]. Rather than the researcher predetermining the topics to be analyzed and coded, however, the topics emerge from corpora of text based on the frequency of co-occurrence (i.e., text-based correlations). Topic modeling has advanced the field of text analysis from identifying specified words through a deductive approach, where topics are pre-identified, to a more inductive approach where meaning is allowed to emerge through a corpus of texts [@mohr13]. 

Topic modeling is still relatively new in the scheme of text-based analytic
research. Blei and colleagues [@blei03] introduced Latent Dirichlet Analysis 
(LDA) in 2003, which has become perhaps the most common topic modeling estimation procedure. Prior to LDA being introduced, inductive themes or latent meanings in text were achievable primarily through qualitative analysis [@nikolenko17]. Topic modeling, however, has potential in an array of contexts, including in combination with sentiment analysis for improved efficiency and accuracy of the sentiment related to consumer comments of products [@lin09], improved understanding of political themes across a range of documents [@hagen18], and the aggregation of results across scientific studies despite differences in terminology and fields of inquiry [@gefen17]. Topic modeling has even shown emerging potential to produce similar results to traditional qualitative frameworks, such as grounded theory, when utilizing a semi-supervised form of LDA in which researchers restricts potential topic assignment to predefined intervals [@nikolenko17]. The increasing accuracy of such techniques therefore provides support for potential use in expanded applications. 

In this paper, we explore the use of topic modeling with LDA estimation to evaluate the textual congruence between test items and content standards as a source of content-related validity evidence. We adopt a similar theoretical framework to alignment studies [@porter02; @webb97; @webb99], but from a text-based machine learning perspective, which may provide an additional triangulating source of  content-related validity evidence. Further, given that the analyses themselves are relatively fast and less cost- and labor-intensive than alignment studies, topic-modeling may be a viable source of diagnostic information during test and item development.

# Method 
We evaluate the textual congruence between the middle school (Grades 6-8) Performance Expectations outlined by the *Next Generation Science Standards* (NGSS), and the Grade 8 Alternate Assessment based on Alternate Achievement Standards (AA-AAS) in Oregon, as described more fully below. In what follows, we describe our data sources, including the NGSS and AA-AAS, as well as our topic model. We specifically detail how we evaluated the number of topics to be extracted, and how the results can be used to evaluate the overall textual congruence between the test items and the content standards. Note that our specific application has some unique aspects related to the measure itself, but the methodology should readily extend to any situation in which an evaluation between a set of items and a set of content standards is needed.

## Data Sources
The NGSS are based on the *Framework for K-12 Science Education* [@nrcframework12] and are built around Performance Expectations, rather than more narrowly-defined standards of academic content knowledge, in each of grades K-5 and grade bands 6-8 (middle school) and 9-12 (high school). NGSS Performance Expectations are situated within one of four Domains (Physical [PS], Life [LS], Earth/Space [ESS], and Engineering Design [ED]) and organized into Arrangement Clusters (groups of interrelated Performance Expectations, e.g., Earth’s Place in the Universe [cluster ESS1]). Together, Performance Expectations unify three dimensions of science learning, requiring that students understand and apply *science and engineering practices* as they master *disciplinary core ideas* and engage *crosscutting concepts*, which span science broadly [@nrc15]. 

Topic models were trained on the text from the middle school NGSS Performance Expectations. A document-term matrix was first produced, wherein each row represented a "document", which in our application was a given Performance Expectation, and the columns represented each unique term from the corpus of documents. The cells contained the frequencies of each term in each document. As part of data processing, we removed stop words (common words like "of", "a", "the", "and", "is") as represented within the *onix*, *SMART*, and *snowball* lexicons [@lewis04; @onix; @snowball] and implemented in the *tidytext* R package [@silge16]. Additionally, we removed verbs associated with Webb's depth of knowledge levels [@webb02]. These removals helped ensure the topics were clustered around content-related words, rather than overly common words or specific verbs prevalent throughout the standards. 

The final document-term matrix included 59 rows, one for each NGSS Performance Expectation, and 355 columns, one for each unique word represented in the content standards that was not a stop word or one of Webb's depth of knowledge verbs. A similar document-term matrix was pre-processed using the same steps as listed above for the test items, using all text represented within each item (i.e., item prompt and three answer options) and each test item serving as a document. Importantly, however, no analyses were conducted on the document-term matrix for items. Rather, the document-term matrix for items was used only to make predictions for each item to the topics derived from the content standards.

## Measures
Our application utilized the science portion of the Grade 8 statewide AA-AAS in Oregon, designed for students with the most significant cognitive disabilities [SWSCDs; @usdoe05]. The AA-AAS is an alternate assessment to the statewide general assessment, with up to 1% of students eligible for reporting purposes (note, there is a 1% reporting cap, not participation cap). SWSCDs are typically characterized by significantly below average general cognitive functioning. Commonly, this includes students with intelligence test scores two or more standard deviations below the mean on a standardized, individually administered intelligence test, occurring with commensurate deficits in adaptive behavior. Further, the cognitive disability must significantly impact the student's educational performance and ability to generalize learning from one setting to another. SWSCDs require highly specialized education and/or social, psychological, and medical services to access an educational program. These students may also rely on adults for personal care and have medical conditions that require physical/verbal supports, and assistive technology devices. These intensive and on-going supports and services are typically provided directly by educators and are delivered across all educational settings, including when administered assessments. Operationally, any student in Oregon with an individualized education program (IEP) is eligible to take the AA-AAS rather than the statewide general assessment, and the recommendation for the most appropriate assessment is guided by the IEP team.

Title 1 Federal Regulations published on December 9, 2003 [@usdoe03], prompted revisions to Oregon's AA-AAS to increase the  accessibility of items by reducing the depth, breadth, and complexity (RDBC) of the content standards (and thus, the associated test items). This essentialization process involved RDBC of the Common Core State Standards (CCSS; in English Language Arts and Mathematics) and the NGSS in order to establish a performance expectation that was relevant and accessible for SWSCDs, while maintaining the high standards of academic rigor. RDBC of content standards was achieved by (a) limiting the scope of content/verbs to (predominantly) *recall* and *application* based on Webb's Depth of Knowledge [webb02], (b) transforming complex process verbs to more basic verbs, and (c) eliminating developmentally inappropriate delimiters and intellectual operations. All essentialized standards were written at three levels of difficulty/cognitive complexity: *Low*, *Medium*, and *High*.

<!-- The RDBC process was in like 2013-14 though... This makes it sound like it was back in like 2004 -->

The Grade 8 AA-AAS in Oregon included 48 items, with 36 used in operational reporting for accountability and 12 used in ongoing field-testing efforts to revise and improve the test annually. Text was included in our analyses from all 48 items. The Oregon AA-AAS is individually administered, with a qualified assessor providing the assessment with IEP-designated supports, and scored using a standardized protocol, with all responses scored dichotomously (correct/incorrect). Students responded to each item through a set of student materials using response modalities commensurate with their IEP and abilities (e.g., verbal, non-verbal, gesturing). 

## Analyses
Our process included (a) using topic modeling to identify key domains/topics within the statewide content standards based on frequency of word co-occurrence; (b) applying the model to new text, in the form of words represented in the test items through either the stem/prompt or the answer options; and (c) evaluating the overall mapping of items within the test items to the modeled domains/topics, including the relative representativeness of each domain/topic within the test. The number of topics was determined through a combination of statistical analyses and expert judgment. The evidence gained from these analyses can then supplement formal alignment studies, while also potentially being useful as a diagnostic tool during test development.

### Topic modeling
Topic modeling is akin to exploratory factor analysis (EFA), where latent variables (topics) are estimated based on the probability that the words within the topic will co-occur [see @mohr13]. As mentioned above, Latent Dirichlet Allocation (LDA), introduced by @blei03, is perhaps the most common estimation procedure for topic models and was the approach used here. LDA is guided by the principles that "every document is a mixture of topics...[and] every topic is a mixture of words" [@silge17, p. 90]. This implies that each document includes different proportions of each modeled topic (including 0% or 100%), and, while the words within a given topic will generally be unique, words can be shared between topics. LDA simultaneously estimates both the mixture of topics within a document and the mixture of words within a topic. The $\beta$ matrix reports  the estimated probability that each word belongs to (or was generated by) a given topic, while the $\gamma$ matrix reports on the probability that each topic is represented within a given document. The "documents" typically represent discretized instances of the overall corpus, such as a collection of blog posts or newspaper articles. As mentioned previously, we treated each NGSS Performance Expectation as a document and evaluated the representativeness of topics within each standard. For example, in a two-topic solution, hypothetical Performance Expectation 1 may be composed of 25% Topic 1 and 75% Topic 2, while hypothetical Performance Expectation 2 is composed of 98% Topic 1 and 2% topic 2. Each topic is then represented by the corpora of words, with each word having a different modeled probability of having been generated by the corresponding topic. Post-hoc substantive labels are generally assigned to topics through expert evaluation of the top $n$ words within each topic (typically 10-20 words), based on the $\beta$ matrix. 

As with EFA, perhaps the most difficult aspect of topic modeling is determining the number of topics (latent factors) to extract, which must be determined *a priori*. Models with different numbers of topics can provide different results and different conclusions about the underlying text. In our application, we relied on a combination of statistical evidence with expert judgment[^1]. From a statistical point of view, we relied upon four tests, as delineated by @arun10, @cao09, @deveaud14, and @griffiths04. Briefly, the method outlined by @arun10 is based on the KL-Divergence of two salient distributions, viewing LDA as a matrix factorization procedure, with the goal of minimizing this value. The @cao09 method relies on topic density through average cosine similarity (minimized), while the @deveaud14 method uses a similar approach but relies on the Jensen-Shannon distance between topic distributions (maximized). Finally, the method proposed by @griffiths04 uses Gibbs sampling with the posterior sampled such that the harmonic mean of the sampled log-likelihoods is maximized. See @houliu18 for a discussion on the performance of these various indicators. We used these tests to evaluate the fit of models with 2 to 25 topics.

[^1]: It is important to note that we consider the topic model preliminary and exploratory, and we welcome feedback and critique from the field. All code used to estimate the models (and produce this manuscript) will be made publicly available upon publication via a GitHub repository.

Following the evaluation of topics, we found a range of topics which appeared to reasonably minimize or maximize each of the criteria (as displayed in the Results section). These topics were then reviewed for substantive meaning and a final topic model was arrived upon through independent evaluations of the topic solutions by two science content experts. This model then represented our final trained model. The probability that each AA-AAS test item was represented by each topic was then estimated. The model was therefore trained on the words within the standards, and we evaluated whether the words used in the items corresponded with the identified topics.

Topics were estimated using the *textmodeling* package [@grun11], while the evaluation of the number of topics to extract was conducted with the *ldatuning* package [@nikita16], both of which are extensions to the R statistical computing environment [@r]. Data were prepared using the *tidyverse* suite of packages [@wickham17], with all plots produced using the *ggplot2* package [@wickham16].

# Results
In this section, we first discuss the selection of the optimal number of topics. We then discuss the mapping of topics to standards, and words to topics. Finally, we present the results of the trained model to the items within the Grade 8 Science portion of the AA-AAS in Oregon, and specifically delineate the results by whether they were written to be of *low*, *medium*, or *high* difficulty. 

## Number of Topics
Figure \@ref(fig:n-topics) displays the optimal number of topics to be extracted across each of the four criteria. As would be expected, the different criteria suggested differing numbers of topics. Relying only on the criteria suggested by @arun10 would lead us to the conclusion that, essentially, the more topics estimated the better the fit, while the opposite conclusion would be reached if only evaluating the results relative to the criteria outlined by @deveaud14. The @cao09 and @griffiths04 criteria appeared more nuanced and suggested differing, but similar ranges of topics. After six topics, however, a marked increase in the @cao09 criteria was observed, indicating a poorer fit, while only a moderate increase (indicating a better fit) was observed with the @griffiths04 criteria. Taken together, these results suggested that between three [the minimum value for the @cao09 criteria] and six topics should be extracted (as displayed by the shaded rectangle in the background of Figure 1). Each topic solution (3-6) was therefore evaluated based upon expert judgment for substantive meaning.

```{r n-topics, fig.cap = "Optimal Topic Selection. Optimal number of topics displayed according to four separate criteria. Shaded rectangle displayed the range in which topics were evaluated for substantive meaning. ", fig.width = 6.5, fig.height = 8}

search_standards <- FindTopicsNumber(dtm, 
                                     topics = 2:25,
                                     metrics = c("Griffiths2004", 
                                                 "CaoJuan2009", 
                                                 "Arun2010", 
                                                 "Deveaud2014"),
                                     method = "Gibbs",
                                     control = list(seed = 77),
                                     mc.cores = 4L)

stats_standards <- apply(search_standards[ ,-1], 
      2, 
      function(x) rescale(x, c(0, 1), range(x))) %>%
  as_tibble() %>%
  rowid_to_column("Topics") %>%
  gather(metric, val, -Topics) %>%
  mutate(criterion = ifelse(metric == "CaoJuan2009" |
                            metric == "Arun2010",
                            "min",
                            "max"),
        metric = case_when(metric == "Griffiths2004" ~ 
                                        "Griffiths & Steyvers, 2004", 
                           metric == "CaoJuan2009" ~ 
                                        "Cao et al., 2009", 
                           metric == "Arun2010" ~
                                        "Arun et al., 2010", 
                           metric == "Deveaud2014" ~
                                        "Deveaud et al., 2014"))

p1 <- ggplot(filter(stats_standards, criterion == "min"),
       aes(Topics, val)) +
  annotate("rect",
           xmin = 2.6, xmax = 6.4,
           ymin = 0, ymax = 1,
           fill = "magenta",
           alpha = 0.2) +
  geom_line(lwd = 1.2, 
            color = "cornflowerblue") +
  geom_point(color = "gray40",
             size = 2) +
  facet_wrap(~metric) +
  labs(x = NULL,
       y = NULL,
       title = "Minimize") +
  theme(plot.title = element_text(hjust = 0.5,
                                  vjust = -3,
                                  face = "bold"),
        panel.spacing = unit(2, "lines"))


p2 <- ggplot(filter(stats_standards, criterion == "max"),
       aes(Topics, val)) +
  annotate("rect",
           xmin = 2.6, xmax = 6.4,
           ymin = 0, ymax = 1,
           fill = "magenta",
           alpha = 0.2) +
  geom_line(lwd = 1.2, 
            color = "cornflowerblue") +
  geom_point(color = "gray40",
             size = 2) +
  facet_wrap(~metric) +
  labs(x = "Number of Topics",
       y = NULL,
       title = "Maximize") +
  theme(plot.title = element_text(hjust = 0.5,
                                  vjust = -3,
                                  face = "bold"),
        panel.spacing = unit(2, "lines"))

p1 / p2
```

Two science content experts with strong science education expertise and familiarity with the NGSS evaluated the 3-6 topic solution models independently for substantive meaning. Differences in labels assigned to topics were resolved through post-hoc collaboration. In general, the content experts began with a three-topic solution and first considered each word within a given topic individually, including their acceptation and known use within the NGSS. Second, the two experts considered their combined meaning within each extracted topic as being a representation of a possible construct (or topic of study) in the broad field of science, while noting the $\beta$ value for each word, and gave each topic a substantive scientific name (see Figure \@ref(fig:word-freq)). Third, and finally, the experts examined each named topic for independence, overlap, and stability across solutions. This process was applied for each topic-solution set between three to six topic solutions.The substantive names between each competing topic-solution were compared for independence, overlap, and stability (i.e., did each topic in the lower solution remain, were new topics created that were independent of others and that had substantive scientific meaning). 

The content experts independently settled on the five-topic solution as best representing the data. Increasing the number of topics from three to four and from four to five led to the addition of substantively new and meaningful topics with little overlap amongst other topics. However, adding a sixth topic led to substantive overlap with at least two other extracted topics, and thus, the five-topic solution was deemed most appropriate. Table \@ref(tab:tbl) displays substantive labels  determined by the two content experts for each of the topics derived from the final (five-topic) model.

```{r tbl, results = "asis"}
tibble(Topic = 1:5,
       `Substantive Label` = 
    c("Analyzing data and using evidence to understand organisms and systems",
      "Using scientific evidence to understand Earth systems",
      "Energy",
      "Genetic information",
      "Scientific and technological solutions")) %>%
papaja::apa_table(caption = 
             "Substantive Labels Assigned to Final Topic Solution")
```

## Mapping Topics to Standards and Words to Topics

```{r finalized-model}
tm_raw <- LDA(dtm, k = 5, control = list(seed = 1234))

betas <- tidy(tm_raw, "beta") 
gammas <- tidy(tm_raw, "gamma") 

ess1_1 <- filter(gammas, document == "S08ESS1.1")
ess2_4 <- filter(gammas, document == "S08ESS2.4")

```

Figure \@ref(fig:heatmap) displays a heatmap of the $\gamma$ values across topics for each of the 59 performance expectations evaluated. Brighter colors represent a higher likelihood of the given topic being reflected by the given standard. For example, Performance Expectation ESS1.1 (bottom) is represented almost entirely (`r paste0(round(ess1_1$gamma[1], 2)*100, "%")`) by Topic 1: *Analyzing data and using evidence to understand organisms and systems*. Performance Expectation ESS2.4, however, is represented partially (`r paste0(round(ess2_4$gamma[1], 2)*100, "%")`) by Topic 1, and partially (`r paste0(round(ess2_4$gamma[2], 2)*100, "%")`) by Topic 2: *Using scientific evidence to understand Earth systems*. These heatmaps can assist in deriving substantive meaning from the topics, given that the performance expectations that predominately make up a topic can be investigated. Once substantive meaning is assigned, however, the topics can likewise help bring new meaning back to the performance expectations, as themes may emerge that were not otherwise  apparent. 

```{r heatmap, fig.cap = "Heatmap of Gamma Values. Cells displayed in brighter colors have higher gamma values, representing a greater likelihood that the given topic is represented by the given standard.", fig.height = 7.5, fig.width = 6.5}
gammas_max <- gammas %>%
  group_by(document) %>%
  filter(gamma == max(gamma)) %>%
  rename(assigned = topic) %>%
  select(-gamma)

gammas <- left_join(gammas, gammas_max) %>%
  mutate(document = gsub("^S08", "", document))

ggplot(gammas, aes(topic, fct_reorder(document, assigned))) +
  geom_tile(aes(fill = gamma),
                color = "gray40") +
  scale_fill_viridis_c(name = "\n\u03B3", 
                       option = "magma",
                       guide = guide_colorbar(
                                  direction = "horizontal",
                                  label.position = "top",
                                  ticks = FALSE,
                                  breaks = seq(0, 1, 0.25),
                                  barwidth = grid::unit(3.5, "in"),
                                  barheight = grid::unit(0.2, "in")),
                       limits = c(0, 1),
                       begin = 0, 
                       end = 1) +
  scale_x_continuous(breaks = 1:5, labels = 1:5) +
  labs(x = "Topic", 
       y = "Performance Expectation") +
  coord_cartesian(expand = FALSE) +
  theme(plot.margin = margin(1.4, 0.2, 0.2, 0.2, "cm"),
        legend.position = c(0.63, 1.05),
        legend.title=element_text(size = 12,
                                  color = "gray40"),
        legend.text=element_text(size = 8,
                                 color = "gray40"),
        axis.text.y = element_text(size = 7),
        legend.key.width = unit(1.2, "cm"))
```

The top 15 words within each topic are displayed in Figure \@ref(fig:word-freq), according to their estimated $\beta$ values. Note that in many instances there were ties among beta values around 15. Those selected for display were chosen randomly. For example, if the 13^th^ to 18^th^ words all had the same $\beta$ value, only the 13^th^ to 15^th^ would be displayed based on a random selection of that $\beta$ value range. Each topic has been labeled according to its identified substantive label. Note that roughly equivalent plots were used for each of the 3-6 topic solutions when arriving upon the final topic model through expert judgment.

```{r word-freq, fig.cap = "Beta Values. Highest probability of words generated by topic. Note the substantive labels were assigned post-hoc. ", fig.width = 12, fig.height = 12}
pd <- betas %>%
  group_by(topic) %>%
  arrange(desc(beta)) %>%
  slice(1:15) %>%
  ungroup() %>%
  mutate(topic = factor(topic, 
                            levels = 1:5,
                            labels = c("Organisms & Systems",
                                       "Earth Systems",
                                       "Energy",
                                       "Genetic Information",
                                       "Scientific and \nTechnological Solutions")),
         reordered = reorder_within(term, beta, topic))

ggplot(pd, aes(reordered, beta)) +
  geom_col(aes(fill = beta),
           alpha = 0.7) +
  coord_flip(clip = "off") +
  scale_y_continuous(breaks = seq(0, 0.09, 0.03),
                     labels = seq(0, 0.09, 0.03)) +
  scale_x_reordered(name = "") +
  scale_fill_distiller(name = "\n\u03b2",
                       type = "seq", 
                       limits = c(0, 0.1),
                       breaks = seq(0, 0.1, 0.02),
                       direction = 1,
                       palette = "Blues",
                       guide = guide_colorbar(
                                 direction = "horizontal",
                                 label.position = "top",
                                 ticks = FALSE,
                                 barwidth = grid::unit(2.75, "in"),
                                 barheight = grid::unit(0.3, "in"))
                       ) +
  facet_wrap(~topic, scales = "free_y") +
  theme(strip.text = element_text(size = 13),
        legend.position = c(0.85, .40),
        legend.title = element_text(size = 20,
                                    color = "gray40"),
        legend.text = element_text(size = 16,
                                   color = "gray40"),
        legend.key.width = unit(1.2, "cm"))
```

## Predicting Items to Topics
In addition to the topic-model providing information about the interrelated nature of the NGSS performance expectations, the final, selected model was also used *predictively*. That is, new text was provided to the model, and topic-level predictions were made. Specifically, each word from the new text was assigned a probability that it was generated from each topic. We used this approach with text from the Grade 8 science items from the Oregon AA-AAS. Importantly, all text from a given item, including the item prompt and the item options, were used when making predictions. These predictions can then be used to evaluate content coverage within the assessment (i.e., coverage of the identified topics).

```{r radar-plot-data}
# Predict the topic for each item
posteriors <- posterior(tm_raw, newdata = idtm)

coord_radar <- function (theta = "x", start = 0, direction = 1) {
 theta <- match.arg(theta, c("x", "y"))
 r <- if (theta == "x") 
        "y"
      else "x"
 ggproto("CoordRadar", CoordPolar, theta = theta, r = r, start = start, 
      direction = sign(direction),
      clip = "off",
      is_linear = function(coord) TRUE)
}

probs <- posteriors$topics %>% 
  as.data.frame() %>% 
  mutate(item = rownames(.)) %>% 
  tbl_df() %>% 
  gather(topic, probability, -item) %>% 
  mutate(Level = str_extract(item, "L|M|H"),
         Level = factor(Level, 
                        levels = c("L", "M", "H"),
                        labels = c("Low", "Medium", "High")),
         topic = factor(topic,
                        levels = 1:5,
                        labels = c("Organisms & Systems",
                                   "Earth Systems",
                                   "Energy",
                                   "Genetic Information",
                                   "Scientific and Technological Solutions")))

```

Figure \@ref(fig:radar-plots-items) displays the probability of the text used within a random sample of nine items being generated from each of the five modeled topics. Random items 2, 5, and 7 all did not include any text that could be classified by our model, and the probability was equally distributed across the five topics. Note that this was expected, and is likely to be a more commonly observed outcome when working with data from AA-AAS assessment systems, given the inherently limiting RDBC process discussed above that intentionally limits textual density within items (particularly for items designed to be of *Low* difficulty). The equal spread of probability across topics is therefore not an indication that the items did not align with the content standards, but rather that the text represented in the item (if any) was not represented within our model. Random Items 4, 6, and 8 all clearly aligned with a single topic, while Random Items 1, 3, and 9 had probabilities split between two topics. Mapping these topics back to the standards they composed therefore provided additional information about the items and their linkage to the content standards.

```{r radar-plots-items, fig.width = 6.5, fig.height = 8, fig.cap = "Probability of topics by item. Random sample of nine items displayed."}
set.seed(3)
samp <- probs %>% 
  sample_n(9)

probs %>% 
  filter(item %in% samp$item) %>% 
  mutate(item = as.numeric(as.factor(item)),
         item = factor(item,
                       levels = 1:9,
                       labels = paste0("Random Item ", 1:9))) %>% 
ggplot(aes(topic, probability, color = item)) +
  geom_polygon(aes(group = Level, fill = item, color = item), alpha = 0.7,
               lwd = 1.3) +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  scale_x_discrete(breaks = c("Organisms & Systems",
                              "Earth Systems",
                              "Energy",
                              "Genetic Information",
                              "Scientific and Technological Solutions"),
                   labels = str_wrap(c("Organisms & Systems",
                                       "Earth Systems",
                                       "Energy",
                                       "Genetic Information",
                                       "Scientific and Technological Solutions"), 
                                     width = 12)) +
  facet_wrap(~item) +
  coord_radar() +
  guides(fill = "none",
         color = "none") +
  labs(x = "",
       y = "") +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_text(size = 7),
        panel.spacing.x = unit(2, "cm"),
        panel.spacing.y = unit(0, "cm"),
        strip.text = element_text(size=10))
```

The model-based predictions from the text used within the items were also used to evaluate content coverage across the test items. In our application, test items were theoretically designed to be of *Low*, *Medium*, and *High* difficulty, as discussed in the methods. We were therefore interested in whether the coverage of the derived topics was equal across each type of item. Figure \@ref(fig:radar-plots-coverage) displays a summary of the overall topic coverage by item type, with the average probability of items representing each of the five topics displayed via radar plots and bar charts. In each display, the thick gray line represents the expected probability if all topics were equally represented (i.e., 20%). Items in the *Low* category, for example, were estimated as slightly under-representing all topics, with the exception of *Organisms and Systems*, which was highly over-represented. This same pattern was present for the items written to be of *Medium* difficulty, although the pattern was even more severe. For the *High* items, the *Genetic Information* topic was the most underrepresented, but overall the coverage was considerably better. Although the evidence was not definitive, this type of information could be useful to guide subsequent investigations of content representativeness. 

```{r radar-plots-coverage, fig.width = 6.5, fig.height = 8, fig.cap="Overall Content Coverage. Gray bands represent the expected probability of topics were equally distributed. Topics are over/underrepresented to the extent the bar/polygon extends above/below the line."}
ref <- tibble(x = c(1:5, 1.05), y = rep(1/5), 6)

radar_summary <- probs %>% 
  group_by(topic, Level) %>% 
  summarize(n = n(),
            prob = mean(probability))  %>% 
  ungroup() %>% 
ggplot(aes(x = topic, y = prob)) + 
  geom_polygon(aes(group = Level, fill = Level, color = Level), alpha = 0.7) +
  geom_path(aes(x, y), ref, color = "gray40", lwd = 1.2) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_discrete(breaks = c("Organisms & Systems",
                              "Earth Systems",
                              "Energy",
                              "Genetic Information",
                              "Scientific and Technological Solutions"),
                   labels = str_wrap(c("Organisms & Systems",
                                       "Earth Systems",
                                       "Energy",
                                       "Genetic Information",
                                       "Scientific and Technological Solutions"), 
                                     width = 12)) +
  facet_wrap(~Level, ncol = 1) +
  coord_radar() +
  annotate("text", 
           x = 3, 
           y = 0.22, 
           label = "20%", 
           color = "gray40", 
           family = "Times New Roman", 
           size = 3) +
  annotate("text", 
           x = 3, 
           y = 0.32, 
           label = "30%", 
           color = "gray40", 
           family = "Times New Roman", 
           size = 3) +
  annotate("text", 
           x = 3, 
           y = 0.42, 
           label = "40%", 
           color = "gray40", 
           family = "Times New Roman", 
           size = 3) +
  annotate("text", 
           x = 3, 
           y = 0.52, 
           label = "50%", 
           color = "gray40", 
           family = "Times New Roman", 
           size = 3) +
  guides(fill = "none",
         color = "none") +
  labs(x = "",
       y = "") +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_text(size = 9),
        strip.text = element_text(size = 0),
        panel.spacing.x = unit(1, "cm"),
        panel.spacing.y = unit(0, "cm"), 
        plot.margin = margin(0, 0, 0, 0, "cm"))

bars_summary <- probs %>% 
  group_by(topic, Level) %>% 
  summarize(n = n(),
            prob = mean(probability))  %>% 
  ungroup() %>% 
ggplot(aes(x = topic, y = prob)) + 
  geom_col(aes(group = Level, fill = Level, color = Level), alpha = 0.7) +
  geom_hline(yintercept = 1/5, color = "gray40", lwd = 1.2) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_discrete(breaks = c("Organisms & Systems",
                              "Earth Systems",
                              "Energy",
                              "Genetic Information",
                              "Scientific and Technological Solutions"),
                   labels = str_wrap(c("Organisms & Systems",
                                       "Earth Systems",
                                       "Energy",
                                       "Genetic Information",
                                       "Scientific and Technological Solutions"), 
                                     width = 5)) +
  facet_wrap(~Level, ncol = 1) +
  geom_text(aes(y = 0, label = str_wrap(topic, width = 5)),
            nudge_y = 0.02,
            vjust = 0,
            family = "Times New Roman",
            size = 2,
            color = "white") +
  theme_void() +
  guides(fill = "none",
         color = "none") +
  theme(strip.text = element_blank(),
        plot.margin = margin(0, 0, 0, 0, "cm")) 

labs <- ggplot(data.frame(x = rep(c(0, 1), 3),
                  y = rep(c(0, 1), 3),
                  diff = factor(rep(c("Low", "Medium", "High"), each = 2),
                                levels = c("Low", "Medium", "High")))) +
  geom_text(x = 0.5, y = 0.5, aes(label = diff), 
            size = 6, 
            family = "Times New Roman")+
  facet_wrap(~diff, ncol = 1) +
  theme_void() +
  theme(strip.text = element_blank())

labs + radar_summary + bars_summary +
  plot_layout(ncol = 3, widths = c(2, 5, 5))

```

# Discussion
Content validity is critical to the overall evaluative judgment of the validity of a test for a given use [@kane06; @messick95] and is a core element examined in alignment studies. This paper introduced a text-based, machine learning method, specifically topic modeling [@mohr13], to evaluate the textual congruence between content standards and test items. This approach has the potential to supplement the methods of current alignment and content-related validity studies by providing a triangulating source of evidence. Text mining also holds potential as a means of learning more about the content standards themselves (i.e., identifying groups of standards through previously unobserved themes), test items, and the textual link between standards and test items, as part of the iterative assessment development and refinement process.

The nature of the topics identified in this analysis is substantively noteworthy. The middle school NGSS curriculum standards are expressed through 59 Performance Expectations that served as one of the primary data sources for this study. These Performance Expectations are based upon (and include each of) the three dimensions of the standards: (a) Scientific and engineering practices, (b) cross-cutting concepts, and (c) disciplinary core ideas [@nrcframework12]. Interestingly, and perhaps appropriately given how the tri-dimensional Performance Expectations were designed to be implemented in science classrooms [@nrc15], the topics we identified reflected a blend of domains (PS, LS, ESS, and ED) and/or dimensions. For example, Topic 1 seemed to emphasize analyzing data (one of the scientific and engineering practices) to understand organisms (a topic reflected in multiple disciplinary core ideas) and systems (a focus of both disciplinary core ideas and cross-cutting concepts). Topic 2 (Using scientific evidence to understand Earth systems) exhibited a similar blend of the three dimensions of the standards. Topic 3, was largely focused on energy, which is reflected in both disciplinary core ideas and cross-cutting concepts. Even Topic 4, which was nearly exclusively focused on genetic information, was strongly associated with the term *evidence*, which is a core part of a scientific and engineering practice. Thus, the topics reflected the way in which the Performance Expectations were intended to blend the three-dimensional framework upon which the NGSS are based. The five extracted topics usefully summarize Performance Expectations that are highly variable in terms of the scientific content they emphasize and the outcomes they target (i.e., a deeper understanding of a disciplinary core ideas; the capacity to engage in a scientific and engineering practice).

Information from topic modeling could be used to supplement data collected through formal alignment studies by triangulating evidence of the alignment for individual items. If the same information was provided by such separate sources, it may help reinforce the overall substantive conclusions about particular items or the test as a whole. If, however, the evidence did not agree, then it may serve as a prompt to collect additional data and conduct further investigations. In addition, the analyses could potentially inform item and test development *during* the developmental process. That is, the analysis could serve as a diagnostic tool to better understand the content coverage of a particular test and when key vocabulary may be missing from a particular item. From a time- and cost-benefit perspective, it is more efficient and cheaper to conduct analyses of data in-house, especially during early phases of development and refinement, than to conduct (post-hoc) formal alignment studies. Part of the benefit of an analytic approach is that the analyses could be conducted much more regularly to inform a truly iterative test documentation and validation process. 

## Limitations and Future Directions
There are several limitations to the approach discussed in this paper that should be kept in mind. First, the results depend highly upon the chosen topic model, including both the number of topics and the substantive meaning assigned to the topics. We view the topic model used here as preliminary, and in need of further external validation. We have therefore published figures similar to Figures \@ref(fig:heatmap) and \@ref(fig:word-freq) for topic solutions between 2 to 16 topics within a GitHub repository that houses all the code for our analyses[^2]. Ideally, the topic model itself could be refined over time, perhaps through a semi-supervised approach to topic modeling [@lu08], that would place predefined constraints on topic assignment based on expert consensus across the field. By making all of our work public, we hope to encourage collaboration and potentially "crowdsource" an optimal model - an approach that has been successfully applied in other fields [e.g., @arganda15; @bentzien13]. Once the model reached a stable point, an Application Programming Interface (API) could be created such that test developers could submit the text from their items and obtain immediate feedback, perhaps in a similar form to the results provided here. The API could be hosted on a secure website or, alternatively, the model itself could be shared through an R package or similar outlet.

[^2]: The repository is currently set as private for peer-review purposes, but will be made public upon publication.

It is also the case that some items, by design in testing contexts like AA-AAS, have little to no text. In this case, our model essentially fails, because it cannot relate the item to any topic, and the probability is equally dispersed across the topics. We view these cases as a flag for further review. In other words, the item may align well with the content standards and contribute meaningfully toward the content-related validity evidence of the test, but not include any text. A more long-term solution would be to utilize other areas of machine learning, such as image recognition. This would include first creating a dataset with items with no text, and providing the standard(s) with which the item was judged as being aligned (through expert judgment). The algorithm could then begin to learn how different image features relate to item alignment. Again, however, this would require the algorithm being iteratively trained, with different algorithms being needed for different content areas. Expert consensus would again need to be established as a source of validity evidence for the algorithm itself. However, if text mining could be used in combination with image recognition (and the image recognition algorithm could be used to learn the topics, so the algorithms would work together), a more powerful content-related validity evidence system could be established. We again, however, would view such a system as diagnostic and a source of triangulating evidence, with human judgment ultimately being the "gold standard". 

Readers may consider what value the topics add in terms of summarizing the Performance Expectations. As noted above, the middle school NGSS Performance Expectations are already grouped into 12 Arrangement Clusters across the four domains. These arrangement clusters represent a more fine-grained description of  performance expectation groupings (relative to the five topics we identified) and are based upon pre-defined and theoretical criteria. These theoretical considerations could potentially be useful within a semi-supervised LDA application. The topics we identified were based solely on the co-occurrence of words within the Performance Expectations, but then reviewed post-hoc for substantive and theoretical meaning. These are alternative approaches to the same basic goal. The topics identified, however, did reflect the contents of the Performance Expectations in a succinct but comprehensive way.

Finally, our specific application utilized the NGSS content standards and a science test. In some ways, science is "easy" to evaluate linkage between tests and items, because key vocabulary must be represented in each. It therefore remains to be seen how this approach would work with other highly-assessed content areas like English language arts and math. Reading would seem to follow rather naturally; for example if a standard discussed verbs, an item may ask students to identify the verb within a sentence. Because the word "verb" would occur in both, the match would be found. However, there are other instances where the mapping between the text used in content standards and items may be disparate, yet aligned. Mathematics may be particularly tricky but, again, more research would be needed. For example, the mathematical symbols themselves could be treated as "words". For topics such as geometry, the image recognition approach discussed above may be more appropriate. 

## Conclusion
Topic modeling Latent Dirichlet Allocation is one of several text-mining procedures that could provide additional information and context to content-related validity studies. The results could also be used diagnostically during item development to help identify areas of under- and over-representation, and particular items that may be missing critical vocabulary. If such an method were applied in operational use, however, it would be critical to communicate the shortcomings of the approach with stakeholders, and that the results not be relied upon in isolation. Attaching numbers and probabilities to words and items may communicate a more "objective" approach, but as discussed above, this is not necessarily the case. As a general diagnostic tool, and as a supplemental source of evidence, however, we believe the approach discussed here holds considerable promise. The practical costs associated with alignment studies are high. Ideally, the alignment study should essentially provide confirmatory evidence, while ad-hoc text analyses could serve as a more diagnostic and exploratory. 

The gathering of content-related validity evidence is dominated by a single method - alignment studies. Machine learning approaches may help to broaden the evidential base. In some cases, specifically in the early grades (pre-K; K-2) tests may not have content standards, and the approach discussed here may seem infeasible. However, formal alignment studies are also challenging, if not impossible, at these levels and, while predictions may not be able to be made toward topics represented in content standards, much could be learned by mining the items themselves and evaluating the topics therein. In sum, while the work presented here is preliminary, text mining and topic modeling as a whole hold considerable promise for the field of measurement and psychometric research, specifically as an alternative means to gathering validity evidence.

\newpage
# References
\begingroup
\setlength{\parindent}{-0.25in}
\setlength{\leftskip}{0.25in}

<div id = "refs"></div>
\endgroup



