---
title: "Evaluating Content-Related Validity Evidence Using Text Modeling"
shorttitle: "Text Modeling Validity"
date: "`r format(Sys.time(), '%B %d, %Y')`"

author: 
  - name          : "Daniel Anderson"
    affiliation   : "1"
    corresponding : yes
    email         : "daniela@uoregon.edu"
    address       : "5262 University of Oregon"
  - name          : "Brock Rowley"
    affiliation   : "1"
    email         : "brockr@uoregon.edu"
  - name          : "Sondra Stegenga"
    affiliation   : "1"
    email         : "sondras@uoregon.edu"
  - name          : "P. Shawn Irvin"
    affiliation   : "1"
    email         : "pirvin@uoregon.edu"
affiliation:
  - id            : "1"
    institution   : "University of Oregon"

bibliography      : refs.bib

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

abstract: |
  The alignment of test items with content standards is a critical source of
  content-related validity evidence within high-stakes testing and
  accountability frameworks. Typically, alignment studies are conducted with
  panels of experts providing qualitative judgments on the degree of alignment
  with each item in the test and the representative content standards. Various
  summary statistics are then calculated from these judgments [e.g.,
  categorical concurrence, balance of representation; @webb99]. In this
  paper, we propose an alternative approach that capitalizes on text-based
  machine learning procedures, specifically topic modeling, to identify
  text-based clusters within the content standards. The probability that each
  item from a statewide assessment aligns with each cluster/topic can then be
  estimated as an additional source of content-related validity evidence. We
  discuss this approach, and show how visualizations can be used to evaluate to
  overall coverage of the content standards by the test items.
class             : "man, fleqn, noextraspace"

header-includes:
    - \raggedbottom
    - \setlength{\parskip}{0pt}
    - \usepackage{xcolor}
output            : 
  papaja::apa6_pdf:
    dev: cairo_pdf
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(rio)
library(here)
library(janitor)
library(tidytext)
library(topicmodels)
library(ldatuning)
library(scales)
library(patchwork)
#devtools::install_github("dgrtwo/drlib")
library(drlib)
library(extrafont)
font_import()

theme_set(theme_minimal(20) +
            theme(strip.text.x = element_text(family = "Times New Roman"),
                  plot.title = element_text(family = "Times New Roman"),
                  axis.text = element_text(color = "gray20",
                                           face = "plain",
                                           family = "Times New Roman"),
                  axis.title = element_text(color = "gray20",
                                            family = "Times New Roman",
                                            face = "plain"),
                  legend.text = element_text(family = "Times New Roman"),
                  legend.title = element_text(family = "Times New Roman")))
```

```{r load_data}
standards <- import(here("data", "8gradescience.xlsx"), 
                    setclass = "tbl_df") %>% 
  clean_names()

webbwords <- import(here("data", "stopwords-webb.xlsx"),
                    setclass = "tbl_df")

items <- import(here("data", "G8_Sci_Items.xlsx"),
                setclass = "tbl_df") %>% 
  clean_names()
```

```{r doc_term_matrices}
# Create the document term matrix for standards
dtm <- standards %>% 
  select(code, ngss_standard) %>% 
  unnest_tokens(word, ngss_standard) %>% 
  anti_join(stop_words) %>% 
  anti_join(webbwords) %>% 
  filter(word != "explanation", # Filter out any additional words we don't want in there
         word != "results") %>% 
  group_by(code) %>% 
  count(word) %>% 
  ungroup() %>% 
  cast_dtm(code, word, n)

# Create a document term matrix for items 
idtm <- items %>% 
  select(item_id, standards, prompt, option_a, option_b, option_c) %>% 
  unite(text, prompt, option_a, option_b, option_c, sep = " ") %>%
  unnest_tokens(word, text) %>%
  filter(word != "a",
         word != "b",
         word != "c") %>%
  anti_join(stop_words) %>% 
  anti_join(webbwords) %>% 
  group_by(item_id) %>% 
  count(word) %>% 
  ungroup() %>% 
  cast_dtm(item_id, word, n)
```

For the last two decades, the landscape of education has evolved through the passage of legislation aimed at improving student outcomes through standards driven accountability such as the No Child Left Behind Act [@nclb02] and its more recent renewal as the Every Student Succeeds Act [see @mcguinn16]. This has led to a vast increase in standardized educational assessment use. As part of this movement an intensive focus on alignment has also emerged due to the need for congruence between assessments, standards, curricula, and instruction to effectively impact student outcomes (e.g. [@porter02]). 

The goal of alignment is to “create a coherent educational system that conveys a clear and unified message about expectations and goals” [@vockley09, p.8]. Alignment studies are commonly conducted with standards-based assessments as a means of evaluating content-related validity evidence, with panels of experts judging the linkage between the content represented in the standards and the content represented in the test items [@sireci07; @webb97]. Content-related validity evidence is one of the five major sources of validity evidence outlined by the *Standards for Educational and Psychological Testing* [@standards14] and is a critical component of the "overall evaluative judgment" [@messick95, p. 741] of the validity of test scores for a given use. However, alignment studies are difficult to execute and design, even under ideal conditions. The chosen methodology, number of participants, professional judgments, consensus through discussion or averaged by ratings, costs associated with travel, and time are elements for consideration [see @anderson15]. Alignment studies are often time consuming and cost intensive. 

According to Kane (2006) content validity is necessarily based on judgments, is more subject to confirmation bias, and lacks the objectivity of other sources of validity evidence, such as criterion-related evidence. This paper adds to the literature on content-related validity evidence through the use of text mining. Specifically, content-related validity evidence is provided by the extent to which test items and content standards display *textual congruence*; that is, to what extent does the text within the items align with the text used in the content standards? We use topic modeling, a text-based machine learning approach, to cluster groups of standards based on frequency of word co-occurrence. These clusters can then be evaluated for substantive meaning, and evaluated in relation to the standards themselves (i.e., whether the empirically-derived clusters match the standards domains or other standards groupings). The extent to which the text within individual test items corresponds with these clusters can be directly evaluated as an additional, alternative source of content-related validity evidence which can supplement, though not replace, information gained from alignment studies.

Topic modeling is a probabilistic method for identifying latent topics in text based documents. Its history stems from qualitative content analysis and latent semantic analysis. Rather than the researcher predetermining the topics to be analyzed and coded, however, the topics emerge from corpora of text based on frequency of co-occurrence (i.e., text-based correlations). Topic modeling has advanced the field of text analysis from identifying specified words through a deductive approach where topics are pre-identified, to a more inductive approach where meaning is allowed to emerge through a corpus of texts [@mohr13]. Topic modeling is still relatively new in the scheme of text-based analytic research. Blei and colleagues [@blei03] introduced Latent Dirichlet Analysis (LDA) in 2003, which has become perhaps the most common topic model estimation procedure. Prior to LDA being introduced, inductive themes or latent meanings in text were achievable primarily through qualitative analysis [@nikolenko17]. Topic modeling, however, has promising potential in a broad array of contexts, including recent studies using LDA in combination with sentiment analysis for improved efficiency and accuracy when examining sentiments of consumer comments about products [@lin09], improved understanding of political themes across a range of documents[@hagen18], and the aggregation of results across scientific studies despite differences in terminology and fields of inquiry [@gefen17]. Topic modeling has even shown emerging potential to produce similar results to traditional qualitative frameworks such as grounded theory on qualitative survey data when utilizing a semi-supervised form of LDA [@nikolenko17]. The increasing accuracy of such techniques therefore provides support for potential use in expanded applications. 

In this paper we explore the use of topic modeling and specifically LDA as a source of content-related validity evidence to evaluate the textual congruence (alignment) between test items and content standards. We adopt a similar theoretical framework to alignment studies [@porter02; @webb97; @webb99], but from a text-based machine learning perspective, which may provide an additional triangulating source of alignment and content-related validity evidence. Further, given that the analyses themselves are relatively fast and less cost- and labor-intensive than alignment studies, topic-modeling may be a viable source of diagnostic information when developing test items.

# Method 
In this section, we describe the measures used for our specific application, as well as the standards they were designed to measure. We then discuss our analysis (topic model) in detail, including how we evaluated the number of topics to be extracted from the standards, and how the results can be used to evaluate the overall textual congruence between the content standards and the test items.

## Content Standards

Science instruction and assessment in Oregon are based on the *Next Generation Science Standards* (NGSS), which were adopted statewide in 2014. The NGSS are are based on the Framework for K-12 Science Education developed by the National Research Council (NRC) and are built around Performance Expectations, rather than more narrowly-defined standards of academic content knowledge, in each of grades K-5, and grade bands 6-8 (middle school) and 9-12 (high school). NGSS Performance Expectations are situated within one of four Domains (Physical [PS], Life [LS], Earth/Space [ESS] and Engineering Design [ED]) and organized into Arrangement Clusters (groups of interrelated Performance Expectations, e.g., Earth’s Place in the Universe [cluster ESS1]). Together, Performance Expectations unify three dimensions of science learning, requiring that students understand and apply *science and engineering practices* as they master *disciplinary core ideas* and engage *crosscutting concepts*, which span science (@nrc15). We evaluate the textual congruence between the the middle school (Grades 6-8) NGSS Performance Expectations, and the Grade 8 Alternate Assessment based on Alternate Achievement Standards (AA-AAS) in Oregon, as described more fully below.
<!-- 
Science Grade 8:
Domain	
LFS	= 9
PHS	= 7
ESS	= 6
ETS	= 2
TOTAL = 24
Note. LFS = Life Science Standards. PHS = Physical Sciences. ESS = Earth and Space Sciences. ETS = Engineering, Technology, and Applications.

The representation ratios (above) can be calculated by dividing the standards by the total within each respective column. For example, in Grade 8 Science, approximately 25% of the items are in the Earth and Space Sciences domain, as that domain has 6 written Essentialized Standards (EsSt) out of the total of 24 (6/24 = 25%).

We evaluate the textual congruence between the Grade 8 *Next Generation Science Standards* (NGSS) and the text used in the AA-AAS item stems and response options, as described in more detail below.
 -->

## Measures
Our application utilized the science portion of the Grade 8 statewide AA-AAAS in Oregon, designed for students with the most significant cognitive disabilities [SWSCDs; @usdoe05]. The AA-AAS is an alternative assessment to the statewide general assessment, with up to 1% of students eligible for reporting purposes (note, there is a 1% reporting cap, not participation cap). SWSCDs are typically characterized by significantly below average general cognitive functioning. Commonly, this includes students with intelligence test scores two or more standard deviations below the mean on a standardized, individually administered intelligence test, occurring with commensurate deficits in adaptive behavior. Further, the cognitive disability must significantly impact the child's educational performance and ability to generalize learning from one setting to another. Students with the most significant cognitive disabilities in general, require highly specialized education and/or social, psychological, and medical services to access an educational program. These students may also rely on adults for personal care and have medical conditions that require physical/verbal supports, and assistive technology devices. These intensive and on-going supports and services are typically provided directly by educators and are delivered across all educational settings. Operationally, any student in Oregon with an individualized education program (IEP) is eligible to take the AA-AAS rather than the statewide general assessment, and the recommendation for the most appropriate assessment is guided by the IEP team.

Title 1 Federal Regulations published on December 9, 2003 [@usdoe03], prompted revisions to Oregon's AA-AAS to increase the  accessibility of items by reducing the depth, breadth, and complexity (RDBC) of the content within the test items. The essentialization process involved RDBC of the Common Core State Standards (CCSS), State Science Standards, and the Next Generation Science Standards (NGSS) in order to establish a performance expectation that was relevant and accessible for SWSCDs, while maintaining the highest possible standards of rigor. Complexity was reduced by: (a) focusing on essential content; (b) simplifying the process verb; and, (c) eliminating inappropriate delimeters. All essentialized standards were written at three levels of complexity: *Low*, *Medium*, and *High*.

## Analyses
Our process included (a) modeling and identifying key domains/topics within the statewide content standards based on frequency of word co-occurrence; (b) applying the model to new text, in the form of words represented in the test items through either the stem/prompt or the answer options; and (c) evaluating the overall mapping of items within the test to the modeled domains/topics, including the relative representativeness of each domain/topic within the test. We consider the evidence gained from these analyses as supplemental to formal alignment studies, and potentially useful as a diagnostic tool during test development.

### Topic modeling
Topic modeling is akin to exploratory factor analysis (EFA), where latent variables  (topics) are estimated based on the probability that the words within the topic will co-occur [see @mohr13]. Latent Dirichlet Allocation (LDA), introduced by @blei03, is perhaps the most common estimation procedure for topic models and was the approach used here. LDA is guided by the principles that "every document is a mixture of topics...[and] every topic is a mixture of words" [@silge17, p. 90]. This implies that each document may include words across a variety of topics, and, while the words within a given topic will generally be unique to the given topic, words can be shared between topics. LDA simultaneously estimates both the mixture of topics within a document and the mixture of words within a topic. The $\beta$ matrix then reports on the estimated probability that each word belongs to (or was generated by) a given topic, while the $\gamma$ matrix reports on the probability that each topic is represented within a given document. The ``documents" typically represent discretized instances of the overall corpus, such as a collection of blog posts or newspaper articles. In our application, we treat each NGSS standard as a document and evaluate the representativeness of topics within each standard. For example, in a two-topic solution hypothetical Standard 1 may be composed of 25% Topic 1 and 75% Topic 2, while hypothetical Standard 2 is composed of 98% Topic 1 and 2% topic 2. The topics themselves are then represented by the corpora of words, with each word having a different modeled probability of having been generated by the corresponding topic. Post-hoc substantive labels are generally assigned to topics through expert evaluation of the top *n* words within each topic (typically 10-20), based on the $\beta$ matrix. 

As with EFA, perhaps the most difficult aspect of topic modeling is determining the number of topics (factors) to extract, which must be determined *a priori*. Models with different numbers of topics can provide different results and different conclusions about the underlying text. In our application, we relied on a combination of statistical evidence with expert judgment[^1]. From a statistical point of view, we relied upon four tests, as delineated by @arun10, @cao09, @deveaud14, @griffiths04. Briefly, the method outlined by @arun10 is based on the KL-Divergence of two salient distributions, viewing LDA as a matrix factorization procedure, with the goal of minimizing this value. The @cao09 method relies on topic density through average cosine similarity (minimized), while the @deveaud14 method uses a similar approach but rely on the Jensen-Shannon distance between topic distributions (maximized). Finally, the method proposed by @griffiths04 uses Gibbs sampling with the posterior sampled such that the harmonic mean of the sampled log-likelihoods is maximized. See @houliu18 for a discussion on performance of these various indicators. We used these tests to evaluate the fit of models with 2 to 25 topics.


[^1]: It is important to note that we consider the topic model preliminary, and we welcome feedback from the field. All code used to estimate the models (and produce this manuscript) are publicly available at the following link: https://github.com/datalorax/text-analysis-content-validity

Following the evaluation of topics, we found a range of topics which appeared to reasonably minimize or maximize each of the criteria (as displayed in the results section). These topics were then reviewed for substantive interpretation and a final topic model was arrived upon through the evaluation of two separate science content experts. This model then represented our final trained model. The probability that each AA-AAS test item was represented by each topic was then estimated. The model was therefore trained on the words within the standards, and we can then evaluate whether the words used in the items correspond to those patterns.

As part of data preprocessing, we removed stop words (common words like "of", "a", "the", "and", "is", etc.) from the *onix*, *SMART*, and *snowball* lexicons [@lewis04; @onix; @snowball], as implemented in the *tidytext* R package [@silge16]. Additionally, we removed verbs associated with Webb's depth of knowledge levels [@webb02]. These  removals helped ensure the topics were clustered around content-related words, rather than specific verbs prevalent throughout the standards, or overly common words. Topics were estimated using the *textmodeling* package [@grun11], while the evaluation of the number of topics to extract was conducted with the *ldatuning* package [@nikita16], both of which are extensions to the  R statistical computing environment [@r]. Data were prepared using the *tidyverse* suite of packages [@wickham17], with all plots produced using the *ggplot2* package [@wickham16], and multiple plots assembled using the *patchwork* package [@pedersen17].

# Results
In this section, we first discuss the selection of the optimal number of topics. We then discuss the mapping of topics to standards, and words to topics. Finally, we present the results of the trained model to the items within the Grade 8 Science portion of the AA-AAS in Oregon, and specifically delineate the results by whether they were written to be of *low*, *medium*, or *high* difficulty. 

## Number of Topics
Figure 1 displays the optimal number of topics to be extracted across each of the four criteria. As would be expected, the different criteria suggested differing numbers of topics. Relying only on the criteria suggested by @arun10 would lead us to the conclusion that, essentially, the more topics estimated the better the fit, while essentially the opposite conclusion would be reached if only evaluating the results relative to the criteria outlined by @deveaud14. The @cao09 and @griffiths04 criteria were more nuanced and suggested differing, but similar ranges of topics. After six topics, however, a marked increase in the @cao09 criteria was observed, indicating a poorer fit, while only a moderate increase (indicating a better fit) was observed with the @griffiths04 criteria. Taken together, these results suggested that between three [the minimum value for the @cao09 criteria] and six topics should be extracted (as displayed by the shaded rectangle in the background of Figure 1). Each of these topic solutions were therefore evaluated for substantive meaning.

```{r n-topics, fig.cap = "Optimal Topic Selection. Optimal number of topics displayed according to four separate criteria. Shaded rectangle displayed the range in which topics were evaluated for substantive meaning. ", fig.width = 6.5, fig.height = 8}

search_standards <- FindTopicsNumber(dtm, 
                                     topics = 2:25,
                                     metrics = c("Griffiths2004", 
                                                 "CaoJuan2009", 
                                                 "Arun2010", 
                                                 "Deveaud2014"),
                                     method = "Gibbs",
                                     control = list(seed = 77),
                                     mc.cores = 4L)

stats_standards <- apply(search_standards[ ,-1], 
      2, 
      function(x) rescale(x, c(0, 1), range(x))) %>%
  as_tibble() %>%
  rowid_to_column("Topics") %>%
  gather(metric, val, -Topics) %>%
  mutate(criterion = ifelse(metric == "CaoJuan2009" |
                            metric == "Arun2010",
                            "min",
                            "max"),
        metric = case_when(metric == "Griffiths2004" ~ 
                                        "Griffiths & Steyvers, 2004", 
                           metric == "CaoJuan2009" ~ 
                                        "Cao et al., 2009", 
                           metric == "Arun2010" ~
                                        "Arun et al., 2010", 
                           metric == "Deveaud2014" ~
                                        "Deveaud et al., 2014"))

p1 <- ggplot(filter(stats_standards, criterion == "min"),
       aes(Topics, val)) +
  annotate("rect",
           xmin = 3, xmax = 6,
           ymin = 0, ymax = 1,
           fill = "magenta",
           alpha = 0.2) +
  geom_line(lwd = 1.2, 
            color = "cornflowerblue") +
  geom_point(color = "gray40",
             size = 3) +
  facet_wrap(~metric) +
  labs(x = NULL,
       y = NULL,
       title = "Minimize") +
  theme(plot.title = element_text(hjust = 0.5,
                                  vjust = -3,
                                  face = "bold"),
        panel.spacing = unit(2, "lines"))


p2 <- ggplot(filter(stats_standards, criterion == "max"),
       aes(Topics, val)) +
  annotate("rect",
           xmin = 3, xmax = 6,
           ymin = 0, ymax = 1,
           fill = "magenta",
           alpha = 0.2) +
  geom_line(lwd = 1.2, 
            color = "cornflowerblue") +
  geom_point(color = "gray40",
             size = 3) +
  facet_wrap(~metric) +
  labs(x = "Number of Topics",
       y = NULL,
       title = "Maximize") +
  theme(plot.title = element_text(hjust = 0.5,
                                  vjust = -3,
                                  face = "bold"),
        panel.spacing = unit(2, "lines"))

p1 / p2
```

\color{red}
**Shawn and/or Josh to provide substantive discussion of why we settled on the number of topics we did. It may make sense to do that here, or it may make more sense (probably does) to embed it within the next section.**
\color{black}

Table 1 displays substantive labels for each of the topics derived from our final topic model.

```{r, results = "asis"}
tibble(Topic = 1:5,
       `Substantive Label` = 
    c("Analyzing data and using evidence to understand organisms and systems",
      "Using scientific evidence to understand natural selection",
      "Energy",
      "Genetic information",
      "Scientific solutions")) %>%
papaja::apa_table(caption = 
             "Substantive Labels Assigned to Final Topic Solution")
```

## Mapping Topics to Standards and Words to Topics


```{r finalized-model}
tm_raw <- LDA(dtm, k = 5, control = list(seed = 1234))

betas <- tidy(tm_raw, "beta") 
gammas <- tidy(tm_raw, "gamma") 

ess1_1 <- filter(gammas, document == "S08ESS1.1")
ess2_4 <- filter(gammas, document == "S08ESS2.4")

```

Figure 2 displays a heatmap of the $\gamma$ values across topics for each of the 59 performance expectations evaluated. Brighter colors represent a higher likelihood of the given topic being represented by the given standard. For example, Performance Expectation ESS1.1 (bottom) is represented almost entirely (`r paste0(round(ess1_1$gamma[1], 2)*100, "%")`) by Topic 1: *Analyzing data and using evidence to understand organisms and systems* . Performance Expectation ESS2.4, however, is represented partially (`r paste0(round(ess2_4$gamma[1], 2)*100, "%")`) by Topic 1, and partially (`r paste0(round(ess2_4$gamma[2], 2)*100, "%")`) by Topic 2: *Using scientific evidence to understand natural selection*. These heatmaps can assist in deriving substantive meaning from the topics, given that the standards that predominately make up a topic can be investigated. Once substantive meaning is assigned, however, the topics can likewise help bring new meaning back to the standards, as themes may emerge that were otherwise not apparent. 


```{r heatmap, fig.cap = "Heatmap of Gamma Values. Cells displayed in brighter colors have higher gamma values, representing a greater likelihood that the given topic is represented by the given standard.", fig.height = 7.5, fig.width = 6.5}
gammas_max <- gammas %>%
  group_by(document) %>%
  filter(gamma == max(gamma)) %>%
  rename(assigned = topic) %>%
  select(-gamma)

gammas <- left_join(gammas, gammas_max)

ggplot(gammas, aes(topic, fct_reorder(document, assigned))) +
  geom_tile(aes(fill = gamma),
                color = "gray40") +
  scale_fill_viridis_c(name = "\nGamma ", 
                       option = "magma",
                       guide = guide_colorbar(
                                  direction = "horizontal",
                                  label.position = "top",
                                  ticks = FALSE,
                                  breaks = seq(0, 1, 0.25),
                                  barwidth = grid::unit(3.5, "in"),
                                  barheight = grid::unit(0.2, "in")),
                       limits = c(0, 1),
                       begin = 0, 
                       end = 1) +
  scale_x_continuous(breaks = 1:5, labels = 1:5) +
  labs(x = "Topic", 
       y = "Standard Code") +
  coord_cartesian(expand = FALSE) +
  theme(plot.margin = margin(1.4, 0.2, 0.2, 0.2, "cm"),
        legend.position = c(0.62, 1.05),
        legend.title=element_text(size = 12,
                                  color = "gray40"),
        legend.text=element_text(size = 8,
                                 color = "gray40"),
        axis.text.y = element_text(size = 7),
        legend.key.width = unit(1.2, "cm"))

```


The top 15 words within each topic are displayed in Figure 3, according to their estimated $\beta$ values. Note that in many instances there were ties among beta values around 15, those selected for display were chosen randomly. For example, if if the 13^th^ to 18^th^ words all had the same $\beta$ value, only the 13^th^ to 15^th^ would be displayed. Each topic has been labeled according to its identified substantive label.


```{r word-freq, fig.cap = "Beta Values. Highest probability of words generated by topic. Note the substantive labels were assigned post-hoc.", fig.width = 12, fig.height = 12}
pd <- betas %>%
  group_by(topic) %>%
  arrange(desc(beta)) %>%
  slice(1:15) %>%
  ungroup() %>%
  mutate(topic = factor(topic, 
                            levels = 1:5,
                            labels = c("Organisms & Systems",
                                       "Natural Selection",
                                       "Energy",
                                       "Genetic Information",
                                       "Scientific Solutions")),
         reordered = reorder_within(term, beta, topic))

ggplot(pd, aes(reordered, beta)) +
  geom_col(fill = "#6272F8",
           alpha = 0.7) +
  coord_flip(clip = "off") +
  scale_y_continuous(breaks = seq(0, 0.09, 0.03),
                     labels = seq(0, 0.09, 0.03)) +
  scale_x_reordered(name = "") +
  facet_wrap(~topic, scales = "free_y") 

# Need to fix this so it shows up as "earth's" instead of "earth..s"

```

## Predicting Items to Topics

In addition to the topic-model providing information about the interrelated nature of the NGSS performance expectations, the finalized model can also be used for predictive purposes. That is, new text can be provided to the model, and topic-level predictions can be made. Specifically, each word from the next text is assigned a probability that it was generated from each topic. We used this approach with text from Grade 8 science items from the AA-AAS used in Oregon. Importantly, all text from a given item, including the item prompt and the item options, were used when making predictions.

```{r radar-plot-data}
# Predict the topic for each item
posteriors <- posterior(tm_raw, newdata = idtm)

coord_radar <- function (theta = "x", start = 0, direction = 1) {
 theta <- match.arg(theta, c("x", "y"))
 r <- if (theta == "x") 
        "y"
      else "x"
 ggproto("CoordRadar", CoordPolar, theta = theta, r = r, start = start, 
      direction = sign(direction),
      clip = "off",
      is_linear = function(coord) TRUE)
}

probs <- posteriors$topics %>% 
  as.data.frame() %>% 
  mutate(item = rownames(.)) %>% 
  tbl_df() %>% 
  gather(topic, probability, -item) %>% 
  mutate(Level = str_extract(item, "L|M|H"),
         Level = factor(Level, 
                        levels = c("L", "M", "H"),
                        labels = c("Low", "Medium", "High")),
         topic = factor(topic,
                        levels = 1:5,
                        labels = c("Organisms & Systems",
                                   "Natural Selection",
                                   "Energy",
                                   "Genetic Information",
                                   "Scientific Solutions")))

```

Figure 4 displays the probability of the text used within a random sample of nine items being generated from each of the five modeled topics. Random items 2, 5, and 7 all did not include any text that could be classified by our model, and the probability was equally distributed across the five topics. Note that this was expected, and is likely to be a more commonly observed outcome when working with data from AA-AAS assessment systems, given the RDBC process discussed above, and that textual density within items is generally  minimized by design. The equal spread of probability across topics is therefore not an indication that the items does not align with the content standards, but rather that the text represented in  the item (if any) was not represented within our model. Random Items 4, 6, and 8 all clearly aligned with a single topic, while random items 1, 3, and 9 had their probability split between two topics. Mapping these topics back to the standards they compose can therefore provide additional information about the items and their linkage to the content standards.

```{r, fig.width = 6.5, fig.height = 8, fig.cap = "Probability of topics by item. Random sample of nine items displayed."}
set.seed(3)
samp <- probs %>% 
  sample_n(9)

probs %>% 
  filter(item %in% samp$item) %>% 
  mutate(item = as.numeric(as.factor(item)),
         item = factor(item,
                       levels = 1:9,
                       labels = paste0("Random Item ", 1:9))) %>% 
ggplot(aes(topic, probability, color = item)) +
  geom_polygon(aes(group = Level, fill = item, color = item), alpha = 0.7,
               lwd = 1.3) +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  scale_x_discrete(breaks = c("Organisms & Systems",
                              "Natural Selection",
                              "Energy",
                              "Genetic Information",
                              "Scientific Solutions"),
                   labels = str_wrap(c("Organisms & Systems",
                                       "Natural Selection",
                                       "Energy",
                                       "Genetic Information",
                                       "Scientific Solutions"), 
                                     width = 12)) +
  facet_wrap(~item) +
  coord_radar() +
  guides(fill = "none",
         color = "none") +
  labs(x = "",
       y = "") +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_text(size = 7),
        panel.spacing.x = unit(2, "cm"),
        panel.spacing.y = unit(0, "cm"),
        strip.text = element_text(size=10))
```

The model-based predictions from the text used within the items can also be used to evaluate the content coverage. In our application, test items were theoretically designed to be of *Low*, *Medium*, and *High*  difficulty, as discussed in the methods. We were therefore interested in if the coverage of the topics was equal across each type of item. Figure 5 displays a summary of the overall topic coverage by item type. Essentially, the average probability of items representing each of the five topics is displayed on the log scale, specifically  $log(p(x_i + 1))$. The thick gray band represents the expected probability, if all topics were equally represented. Items in the low category, for  example, were estimated as slightly under-representing all topics, with the exception of *Organisms and Systems*, which was highly over-represented. This same pattern was present for the items written to be of *Medium* difficulty, although it was even more severe. For the *High* items, the *Genetic Information* topic was underrepresented, but overall the coverage looks considerably better. Although the evidence is not definitive, this type of information could potentially be used to guide subsequent investigations of content representativeness. 

```{r, fig.width = 6.5, fig.height = 12, fig.cap="Overall Content Coverage"}
ref <- tibble(x = c(1:5, 1.05), y = rep(0, 6))

probs %>% 
  group_by(topic, Level) %>% 
  summarize(n = n(),
            prob = sum(probability))  %>% 
  mutate(prob = log((prob/n)*5)) %>% 
  ungroup() %>% 
ggplot(aes(x = topic, y = prob)) + 
  geom_polygon(aes(group = Level, fill = Level, color = Level), alpha = 0.7) +
  geom_path(aes(x, y), ref, color = "gray40", lwd = 1.2) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_discrete(breaks = c("Organisms & Systems",
                              "Natural Selection",
                              "Energy",
                              "Genetic Information",
                              "Scientific Solutions"),
                   labels = str_wrap(c("Organisms & Systems",
                                       "Natural Selection",
                                       "Energy",
                                       "Genetic Information",
                                       "Scientific Solutions"), 
                                     width = 12)) +
  facet_wrap(~Level) +
  coord_radar() +
  guides(fill = "none",
         color = "none") +
  labs(x = "",
       y = "") +
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_text(size = 7),
        panel.spacing.x = unit(1, "cm"),
        panel.spacing.y = unit(0, "cm"))
```

# Discussion

\color{red} Note from DA: I haven't touched this section yet \color{black}

Content validity is critical to the overall evaluative judgment of the validity of a test for a given use. This paper introduces a new method using text mining procedures to evaluate the concurrence between language used in the content standards and language used in the test items. From a cost-benefit perspective, it is much cheaper to conduct an analysis of data in-house than to conduct alignment studies. These analyses could even be conducted during item and test development to inform the developmental process. However, the analyses are not intended to *replace* the evidence gathered during alignment studies, but rather to *supplement*. Part of the benefit of the analytic approach, however, is that they could be conducted much more regularly to inform the iterative  test documentation/validation process. 

It should also be noted that our analysis and results presented here are preliminary. Before the conference, we plan to obtain feedback from content experts to verify or provide guidance on modifications to our trained model, given that the validity of the procedure depends on the validity of the trained model (i.e., all the topics make sense and sufficient topics are extracted to cover the content represented in the standards). From a bigger-picture perspective, however, because these models are based on the standards, rather than any individual items, the models themselves could be made public with  other provided the opportunity to provide input. Further, they could actually apply the model to their own data to evaluate content coverage or topic concurrence for individual items in a manner similar to that shown here. For  the conference paper, we plan to provide much more detail about the modeling, its strengths and limitations, and a more in-depth illustrations of the results of our application.

<!-- Overall, this paper will discuss a new and innovative proposed approach to establishing validity through analyzing and categorizing text data via modern data tools of R and RStudio text analysis and structural topic modeling. It does not replace current methods for establishing validity but demonstrates initial promise to add strength to the development design. In a world of constantly evolving and growing data and data sources it is imperative as educational researchers that we not only begin to explore new methods that hold promise for increasing efficiency and accuracy with data analysis but also ensure that methods engage an element of translatability. By increasing the accuracy of constructs and improved validity we provide the opportunity to increase utility and translatability over a range of consumers in the educational community. In addition, modern technology provides an array of methods and open source resources and tools, such as R and RStudio, that not only provide the ability to capture and categorize the data but also visualize the findings. Again, this is an imperative piece in a world that has seen vastly increased calls for the translation of data and research to practice and practice to research. -->

\newpage
# References
\begingroup
\setlength{\parindent}{-0.25in}
\setlength{\leftskip}{0.25in}

<div id = "refs"></div>
\endgroup



